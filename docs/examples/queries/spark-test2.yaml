# Dataproc cluster configuration for Spark analytics workloads
cluster:
  name: spark-analytics-cluster
  region: us-central1
  config:
    gceClusterConfig:
      # Example using a shared VPC setup
      subnetworkUri: projects/your-project-id/regions/us-central1/subnetworks/your-subnet-name
      serviceAccount: your-dataproc-service-account@your-project-id.iam.gserviceaccount.com
      tags:
        - allow-google-apis
        - allow-iap-ssh
        - dataproc-vm
      metadata:
        # Example metadata for custom artifacts
        artifact_urls: "gs://your-bucket/artifacts/analytics-tools.zip"
        environment: "production"
    masterConfig:
      numInstances: 1
      machineTypeUri: e2-standard-4
    workerConfig:
      numInstances: 3
      machineTypeUri: e2-standard-2
    softwareConfig:
      imageVersion: 2.2-debian
      optionalComponents:
        - ZEPPELIN
        - JUPYTER
      properties:
        # Spark configuration optimizations
        "spark:spark.sql.adaptive.enabled": "true"
        "spark:spark.sql.adaptive.coalescePartitions.enabled": "true"
    initializationActions:
      - executableFile: gs://your-bucket/init/setup-environment.sh
      - executableFile: gs://your-bucket/init/install-dependencies.sh
    endpointConfig:
      enableHttpPortAccess: true
    metastoreConfig:
      # Optional: Use external Hive Metastore
      dataprocMetastoreService: projects/your-project-id/locations/us-central1/services/your-metastore-service
    lifecycleConfig:
      # Auto-delete cluster after 100 minutes of inactivity
      idleDeleteTtl: 6000s
    # Security configuration
    securityConfig:
      kerberosConfig:
        enableKerberos: false